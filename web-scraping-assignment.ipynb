{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Popular Blog Details on SvN Using Python\n",
    "\n",
    "![banner_image](https://i.imgur.com/UlSaW5R.png)\n",
    "\n",
    "SvN(Signal V Noise) was a popular blog site where blogs on design, business, and tech were regularly posted till 2021 from various authors. It was designed by the the makers (and friends) of [Basecamp](https://www.basecamp.com/).\n",
    "\n",
    "The page https://m.signalvnoise.com/search/ provides month wise blog posts from February 2021 to November 2013. In this assignment, we will retrieve information from this page using _web_scraping_: the process of extracting information from a website in an automated fashion using code. We will use [Requests](https://requests.readthedocs.io/en/latest/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to scrap data from this page.\n",
    "\n",
    "The outline of this assignment is listed below:\n",
    "\n",
    "1. Download the webpage using `requests`\n",
    "2. Parse the HTML source code using `beautiful soup`\n",
    "3. Extract Blog name, author name, published date and blog URLs from this page\n",
    "4. Compile extracted information using Python lists\n",
    "5. Save the extracted information to a CSV file.\n",
    "\n",
    "The CSV file which will be created will have the following format:\n",
    "```\n",
    "Blog Name,Author Name,Published date,Blog URL\n",
    "Things are going so well weâ€™re doing a hiring freeze,DHH,JANUARY 31, 2018,https://m.signalvnoise.com/things-are-going-so-well-were-doing-a-hiring-freeze/\n",
    "Making It Personal,NATHAN KONTNY,JANUARY 31, 2018,https://m.signalvnoise.com/making-it-personal/\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can execute the code using the \"Run on Binder\" button at the top of this page. You can make changes and save your own version of the notebook to [Jovian](https://jovian.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"web-scraping-assignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"rahulajvit/web-scraping-assignment\" on https://jovian.com\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.com/rahulajvit/web-scraping-assignment\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.com/rahulajvit/web-scraping-assignment'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project = project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the webpage using `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `requests` library to download the webpage.\n",
    "\n",
    "The library can be installed using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download a page, we can use the `get` function from requests, which returns the response object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_search_url = 'https://m.signalvnoise.com/search/'\n",
    "response = requests.get(blog_search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests.get` returns a response object containing the data from the webpage and some other information.\n",
    "\n",
    "The `.status_code` property can be used to check if the request was successful.A successful response will have an HTTPstatus code between 200 and 209."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the number of characters in the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26936"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents = response.text\n",
    "len(page_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The webpage contains over 25,000 characters. Here are the first 1000 characters of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<html lang=\"en-US\">\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\t<link rel=\"profile\" href=\"https://gmpg.org/xfn/11\">\\n\\n\\t<meta name=\\'robots\\' content=\\'index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1\\' />\\n<!-- Jetpack Site Verification Tags -->\\n<meta name=\"google-site-verification\" content=\"0DhapiM4O0UOfgAG0h7WKwcen-OnbamAZvubxFltbwE\" />\\n<meta name=\"msvalidate.01\" content=\"76FA9A6899F84C58C463EBD43BD0827E\" />\\n\\n\\t<!-- This site is optimized with the Yoast SEO plugin v19.14 - https://yoast.com/wordpress/plugins/seo/ -->\\n\\t<title>Search SvN - Signal v. Noise</title>\\n\\t<meta name=\"description\" content=\"Looking for a specific post on SvN? Search for it here.\" />\\n\\t<link rel=\"canonical\" href=\"https://m.signalvnoise.com/search/\" />\\n\\t<script type=\"application/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https://schema.org\",\"@graph\":[{\"@type\":\"WebPage\",\"@id\":\"https://m.signalvnoise.com/search/\",\"url'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is the [HTML source code](https://en.wikipedia.org/wiki/HTML) of the webpage. We can also save it to a file and view locally within Jupyter using \"File -> Open\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('webpage.html','w') as f:\n",
    "    f.write(page_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page looks similiar to the original but none of the links will work in this webpage.\n",
    "\n",
    "![](https://i.imgur.com/XQOY9uZ.png)\n",
    "\n",
    "In this section, we have successfully used the `requests` library to download a webpage as HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"rahulajvit/web-scraping-assignment\" on https://jovian.com\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.com/rahulajvit/web-scraping-assignment\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.com/rahulajvit/web-scraping-assignment'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project = project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the HTML source code using `beautiful soup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `BeautifulSoup` module from `bs4` library to parse the html code which was obtained using the `requests` library.\n",
    "\n",
    "The library can be installed using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse HTML contents of a webpage, we can pass the HTML contents to the `BeautifulSoup` class along with indication of `html parser` which returns a bs4 object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(page_contents,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Search SvN - Signal v. Noise</title>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine this step and the previous step to write a function that takes the blog_search_url variable as an argument which returns a `bs4` object which can be later used for scraping the required information.\n",
    "\n",
    "Let's see the title of the `BeautifulSoup` doc using the webpage URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(url):\n",
    "    \"\"\"Download a webpage and return a BeautifulSoup doc\"\"\"\n",
    "    #Download the webpage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #Check if the download was successful\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Unable to download page {}'.format(url))\n",
    "    \n",
    "    #Get the page HTML\n",
    "    html_contents = response.text\n",
    "    \n",
    "    #Create a bs4 doc\n",
    "    doc = BeautifulSoup(html_contents,'html.parser')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will call the function `get_pages` by passing the required URL as argument and verify the title of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Search SvN - Signal v. Noise</title>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = get_pages(blog_search_url)\n",
    "doc.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the function output and then printing the title of the `bs4` object, we can confirm the function has the same usage as the 1st and 2nd steps which we wrote before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we have successfully used the `BeautifulSoup` module from the `bs4` library to parse the HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"rahulajvit/web-scraping-assignment\" on https://jovian.com\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.com/rahulajvit/web-scraping-assignment\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.com/rahulajvit/web-scraping-assignment'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project = project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Blog name, author name, published date and blog URLs from the webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the details of the blogs published in `February 2021` and then we can extend it to obtain the CSV file information for other months and years by writing a function.\n",
    "\n",
    "But to extract the the details of the published blogs in a month, we need the URLs to navigate to a particular month and year combination. \n",
    "\n",
    "Let's see under which tags of the HTML code the blog URL specific for a particular month exists.\n",
    "\n",
    "![](https://i.imgur.com/5DTdKvl.png)\n",
    "\n",
    "From the above image, we can see that under the `ul` tag and class type `a`, the specific blog month URL can be scraped.\n",
    "\n",
    "Let's define an empty list which would contain URLs specific to unique month and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_month_links = []\n",
    "ul_tags = doc.find('ul')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URLs specific to the list of blogs published in particular month and year is obtained from the `bs4 doc` and they are are contained in the `ul` class of the `bs4 doc`. So, we will use the `find` method to obtain the `ul_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li in ul_tags.find_all('a', href=True):\n",
    "    blog_month_links.append(li['href']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate through the `ul_tags` and search for `href` tags of class type `a`  in order to obatin the URLs specific to each month.\n",
    "\n",
    "Since,`February 2021` occurs first in the https://m.signalvnoise.com/search/, its URL will be the first item in `blog_month_links`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://m.signalvnoise.com/2021/02/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_month_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From seeing the 1st item in the `blog_month_links` list , we can confirm the URL specific to `February 2021`.\n",
    "\n",
    "We will store this URL in a variable `feb21_link`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://m.signalvnoise.com/2021/02/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb21_link = blog_month_links[0]\n",
    "feb21_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass the URL in the variable `feb21_link` to the function `get_pages` as an argument and verify the title of the webpage.\n",
    "\n",
    "The `bs4 doc` returned from the function is stored in the variable `feb21_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>February 2021 - Signal v. Noise</title>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb21_doc = get_pages(feb21_link)\n",
    "feb21_doc.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's define a function `blog_details_extracter`. \n",
    "\n",
    "This function will take the `bs4` doc of a URL passed from the `blog_month_links` list as an argument and would return the blog titles, author names, published dates and blog URLs as a dictionary with keys - `'Blog Name'`, `'Author'`,`'Published date'`and `'Blog Link'` and the values for each key will be a list containing the respective details specific to that key. Then they can be later used to form a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blog_details_extracter(beautifulsoup_doc):\n",
    "    \"\"\"To return the blog titles, author names, published dates and blog URLs as 4 lists \n",
    "        from the bs4 object that is passed as an argument\"\"\"\n",
    "    \n",
    "    #Get all article tags with a specific class attribute which contains the required CSV contents. \n",
    "    article_tags = beautifulsoup_doc.find_all('article',class_='entry-summary grid__item grid__item--third')\n",
    "    \n",
    "    #Initialise the 4 lists for storing blog titles, author names, published dates and blog URLs. \n",
    "#     blog_titles,blog_authors,blog_published_dates,blog_links = [],[],[],[]\n",
    "    \n",
    "    #Initialise a dictionary for resultant blog details.\n",
    "    blog_details = {'Blog Name':[],'Author':[],'Published date':[],'Blog Link':[]}\n",
    "    \n",
    "    #Iterate through the article_tags list to fin the required information\n",
    "    for li in article_tags:\n",
    "        \n",
    "        #Blog titles are part of the first a-tag \n",
    "        blog_details['Blog Name'].append(li.find('a', href=True).find_next(text=True).get_text(strip=True))\n",
    "        \n",
    "        #Blog authors are part the a-tags with the class type - 'author url fn'\n",
    "        blog_details['Author'].append(li.find('a',{\"class\": \"author url fn\"}, href=True).find_next(text=True).get_text(strip=True))\n",
    "        \n",
    "        #Blog published dates are part the time-tags with the class type - 'entry-date published updated'\n",
    "        blog_details['Published date'].append(li.find('time', class_=\"entry-date published updated\", href=False).find_next(text=True).get_text(strip=True))\n",
    "        \n",
    "        #Blog URL links are part the a-tags(href attribute) with the class type - 'entry-date published updated'\n",
    "        blog_details['Blog Link'].append(li.find('a').get(\"href\"))\n",
    "        \n",
    "    #Assigning the scraped blog titles list to the key 'Blog name' in the dictionary blog_details\n",
    "#     blog_details['Blog Name'] = blog_titles\n",
    "    \n",
    "    #Assigning the scraped blog authors list to the key 'Author' in the dictionary blog_details\n",
    "#     blog_details['Author'] = blog_authors\n",
    "    \n",
    "    #Assigning the scraped blog published dates list to the key 'Published date' in the dictionary blog_details\n",
    "#     blog_details['Published date'] = blog_published_dates\n",
    "    \n",
    "    #Assigning the scraped blog URLs list to the key 'Blog Link' in the dictionary blog_details\n",
    "#     blog_details['Blog Link'] = blog_links\n",
    "    \n",
    "    return blog_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking the functionality of the function `blog_details_extracter`, let's obtain the details of blog titles, author names, published dates and blog URLs for `February 2021`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb21_blogs_details = blog_details_extracter(feb21_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the above variables and compare with the screenshot of the original webpage shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog titles published in Feb 2021 are Testimony before the North Dakota Senate Industry, Business and Labor Committee\n",
      "Authors who published blogs in Feb 2021 are DHH\n",
      "The dates of the blogs in Feb 2021 are February 9, 2021\n",
      "The links to the blogs published in Feb 2021 are https://m.signalvnoise.com/testimony-before-the-north-dakota-senate-industry-business-and-labor-committee/\n"
     ]
    }
   ],
   "source": [
    "print('Blog titles published in Feb 2021 are',feb21_blogs_details['Blog Name'][0])\n",
    "print('Authors who published blogs in Feb 2021 are',feb21_blogs_details['Author'][0])\n",
    "print('The dates of the blogs in Feb 2021 are',feb21_blogs_details['Published date'][0])\n",
    "print('The links to the blogs published in Feb 2021 are',feb21_blogs_details['Blog Link'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the webpage specific to `February 2021`.\n",
    "\n",
    "![](https://i.imgur.com/BSYpgoI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above `screenshot` and `print` statements we can verify that we have extracted required information from the URL specific to `February 2021`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we have used the `bs4 doc` and extracted the required the required contents specific to a month and which will be later extended to obtain the required contents for all months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile extracted information using Python lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the required details for the `csv` file by passing the `bs4 objects` to `blog_details_extracter` function to get a dictionary containing values of lists of blog titles,blog authors,blog published dates,blog links from `February 2021` to `November 2013`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_blog_details = {'Blog Name':[],'Author':[],'Published date':[],'Blog Link':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above step, we have created a empty dictionary to store all the blog titles, author names, blog published dates and blog URLs as values for the corresponding keys in `all_blog_details` dictionary from `Feb 21` to `Nov 13`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a `for` loop that iterates through all the `blog_month_links` and then returns the dictionary with values as the total list of blog titles, author names, published dates and blog URLs from `Feb 21` to `Nov 13`, monthwise for the respective keys.\n",
    "\n",
    "Then details specific to each month are added in the `all_blog_details` dictionary defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_blog_details(link):\n",
    "    \n",
    "    all_blog_details = {'Blog Name':[],'Author':[],'Published date':[],'Blog Link':[]}\n",
    "    \n",
    "    for i in range(len(link)):\n",
    "    \n",
    "        bs4_object = get_pages(link[i])\n",
    "    \n",
    "        blog_details = blog_details_extracter(bs4_object)\n",
    "    \n",
    "        all_blog_details['Blog Name'] += blog_details['Blog Name']\n",
    "        all_blog_details['Author'] += blog_details['Author']\n",
    "        all_blog_details['Published date'] += blog_details['Published date']\n",
    "        all_blog_details['Blog Link'] += blog_details['Blog Link']\n",
    "    \n",
    "    return all_blog_details\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blog_details = scrap_blog_details(blog_month_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we have extracted the required `csv` contents from `Feb 21` to `Nov 13`. First, we obtain the `bs4 object` specific to a URL using the `get_pages` function and pass the `bs4 object` to the function `blog_details_extracter` to obtain the required details for the csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the extracted information to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the the above dictionary containing the lists of contents as key-value pairs, we have to merge them into a single `dataframe` using `pandas` library and then we can to save the contents to `csv` file using the `DataFrame` method of `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install the `pandas` library and import it as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the `all_blog_details` dictionary into a pandas dataframe `df` and view the first 5 rows of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blog Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Published date</th>\n",
       "      <th>Blog Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Testimony before the North Dakota Senate Indus...</td>\n",
       "      <td>DHH</td>\n",
       "      <td>February 9, 2021</td>\n",
       "      <td>https://m.signalvnoise.com/testimony-before-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reiterating our Use Restrictions Policy</td>\n",
       "      <td>Jason Fried</td>\n",
       "      <td>January 18, 2021</td>\n",
       "      <td>https://m.signalvnoise.com/reiterating-our-use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTML over the wire</td>\n",
       "      <td>DHH</td>\n",
       "      <td>December 23, 2020</td>\n",
       "      <td>https://m.signalvnoise.com/html-over-the-wire/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Validation is a mirage</td>\n",
       "      <td>Jason Fried</td>\n",
       "      <td>December 22, 2020</td>\n",
       "      <td>https://m.signalvnoise.com/validation-is-a-mir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Making of a Dumpster Fire</td>\n",
       "      <td>Andy Didorosi</td>\n",
       "      <td>December 15, 2020</td>\n",
       "      <td>https://m.signalvnoise.com/the-making-of-a-dum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Blog Name         Author  \\\n",
       "0  Testimony before the North Dakota Senate Indus...            DHH   \n",
       "1            Reiterating our Use Restrictions Policy    Jason Fried   \n",
       "2                                 HTML over the wire            DHH   \n",
       "3                             Validation is a mirage    Jason Fried   \n",
       "4                      The Making of a Dumpster Fire  Andy Didorosi   \n",
       "\n",
       "      Published date                                          Blog Link  \n",
       "0   February 9, 2021  https://m.signalvnoise.com/testimony-before-th...  \n",
       "1   January 18, 2021  https://m.signalvnoise.com/reiterating-our-use...  \n",
       "2  December 23, 2020     https://m.signalvnoise.com/html-over-the-wire/  \n",
       "3  December 22, 2020  https://m.signalvnoise.com/validation-is-a-mir...  \n",
       "4  December 15, 2020  https://m.signalvnoise.com/the-making-of-a-dum...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(all_blog_details)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the shape of the `dataframe` using the `shape` function of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will convert `df` to a `csv` file called `blogs.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('blogs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"rahulajvit/web-scraping-assignment\" on https://jovian.com\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.com/rahulajvit/web-scraping-assignment\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.com/rahulajvit/web-scraping-assignment'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project = project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we covered in this notebook:\n",
    "\n",
    "1. Download the webpage using `requests`\n",
    "2. Parse the HTML source code using `beautiful soup`\n",
    "3. Extract Blog name, author name, published date and blog URLs from webpage\n",
    "4. Compile extracted information using Python lists\n",
    "5. Save the extracted information to a CSV file.\n",
    "\n",
    "The CSV file which will be created will have the following format:\n",
    "```\n",
    "Blog Name,Author,Published date,Blog Link\n",
    "\"Testimony before the North Dakota Senate Industry, Business and Labor Committee\",DHH,\"February 9, 2021\",https://m.signalvnoise.com/testimony-before-the-north-dakota-senate-industry-business-and-labor-committee/\n",
    "Reiterating our Use Restrictions Policy,Jason Fried,\"January 18, 2021\",https://m.signalvnoise.com/reiterating-our-use-restrictions-policy/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can fetch the details about how many blogs were published in each moth listed in the homepage.\n",
    "* We can fetch the details about the comments which were seen in some blogs - number of comments and details about the them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://requests.readthedocs.io/en/latest/\n",
    "2. https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "3. https://stackoverflow.com/questions/17246963/how-to-find-all-lis-within-a-specific-ul-class\n",
    "4. https://www.geeksforgeeks.org/beautifulsoup-find-all-li-in-ul/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit(project = project_name, files = ['blogs.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}